{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78171b33-6ea2-4c0b-84e6-6765e7f1e6b2",
   "metadata": {},
   "source": [
    "# Python Tutorial: Scikit-Learn (sklearn)\n",
    "\n",
    "Scikit-learn (sklearn) is a popular machine learning library in Python that provides various tools for building and applying machine learning models. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301826ef-014c-4ccf-af59-c558cb94b337",
   "metadata": {},
   "source": [
    "## Official Documentation\n",
    "\n",
    "https://scikit-learn.org/stable/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0654dfe-bd68-4b4e-95dc-0f2043033b5c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Overview</b></summary>\n",
    "\n",
    "Sklearn offers a wide range of models, which can be broadly categorized into the following types based on the learning task:\n",
    "\n",
    "1. **Supervised Learning Models:**\n",
    "   In supervised learning, the algorithm learns from labeled data, meaning the input data is accompanied by corresponding output labels. Supervised learning models in sklearn can be further divided into two subcategories:\n",
    "\n",
    "   - **Classification Models:** These models are used for predicting categorical labels. Examples include Logistic Regression, Decision Trees, Random Forest, Support Vector Machines (SVM), k-Nearest Neighbors (kNN), etc.\n",
    "   \n",
    "   - **Regression Models:** Regression models are used for predicting continuous values. Examples include Linear Regression, Ridge Regression, Lasso Regression, Support Vector Regression (SVR), etc.\n",
    "\n",
    "2. **Unsupervised Learning Models:**\n",
    "   In unsupervised learning, the algorithm learns patterns and structures from unlabeled data. Unsupervised learning models in sklearn include:\n",
    "\n",
    "   - **Clustering Models:** These models are used for grouping similar data points into clusters based on some similarity measure. Examples include K-Means, Hierarchical Clustering, DBSCAN, etc.\n",
    "   \n",
    "   - **Dimensionality Reduction Models:** These models are used for reducing the number of features in the data while preserving important information. Examples include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), etc.\n",
    "\n",
    "3. **Semi-Supervised Learning Models:**\n",
    "   Semi-supervised learning combines both labeled and unlabeled data to improve learning accuracy. Sklearn provides some semi-supervised learning algorithms, including LabelPropagation and LabelSpreading.\n",
    "\n",
    "4. **Model Selection and Evaluation:**\n",
    "   Sklearn also provides tools for model selection and evaluation, including:\n",
    "\n",
    "   - **Cross-Validation:** Techniques like k-fold cross-validation, which splits the dataset into k subsets and trains the model k times, each time using a different subset as the test set.\n",
    "   \n",
    "   - **Model Evaluation Metrics:** Sklearn offers various metrics to evaluate model performance, such as accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression.\n",
    "   \n",
    "   - **Hyperparameter Tuning:** GridSearchCV and RandomizedSearchCV are used for finding the best hyperparameters for a model by exhaustively searching through a specified parameter grid or randomly sampling from a parameter distribution.\n",
    "\n",
    "5. **Ensemble Methods:**\n",
    "   Ensemble methods combine multiple individual models to improve performance. Sklearn provides ensemble methods like Random Forest, Gradient Boosting, AdaBoost, etc.\n",
    "\n",
    "These are some of the major model types and functionalities offered by sklearn. Each model type has its strengths and weaknesses, and choosing the right model depends on the specific problem at hand and the characteristics of the dataset.\n",
    "                                                                                                 </details>                                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196da9e1-262a-4520-864e-da044f241037",
   "metadata": {},
   "source": [
    "## Installation\n",
    "  \n",
    "You can install scikit-learn using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c3fd1-76a0-4541-bf4f-300285b3ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6f521-0304-458a-8961-26f59ab92a15",
   "metadata": {},
   "source": [
    "## Example 1: Linear Regression\n",
    "\n",
    "Linear Regression is a simple and widely used supervised learning algorithm for predicting continuous values. It establishes a linear relationship between the independent variables (features) and the dependent variable (target). In sklearn, linear regression is implemented in the `LinearRegression` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b4447-3508-419c-8ea1-5901e62cbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Random Data for Linear Regression')\n",
    "plt.show()\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "# `X` is a 2-dimensional array-like object containing the training data (features).\n",
    "# `y` is a 1-dimensional array-like object containing the target values corresponding to `X`.\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "# `X_new` is a 2-dimensional array-like object containing the test data (features).\n",
    "# `y_pred` will contain the predicted values corresponding to `X_new`.\n",
    "X_new = np.array([[0], [2]])\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# Visualize the linear regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_new, y_pred, color='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc803072-2165-491c-9cdf-075d47827093",
   "metadata": {},
   "source": [
    "## Example 2: Classification with Support Vector Machine (SVM)\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. In the context of classification, SVM aims to find the optimal hyperplane that best separates different classes in the feature space. Sklearn provides an implementation of SVM for classification tasks in the `SVC` (Support Vector Classification) class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e408b12-b6d7-46ab-a466-8c375056f48e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>How SVM Classification Works</b></summary>\n",
    "    \n",
    "1. **Maximum Margin Classifier:**\n",
    "   SVM seeks to find the hyperplane that maximizes the margin, i.e., the distance between the hyperplane and the closest data points (support vectors) from each class. This hyperplane effectively creates decision boundaries between different classes.\n",
    "\n",
    "2. **Kernel Trick:**\n",
    "   SVM can efficiently handle non-linearly separable data by using the kernel trick. Instead of explicitly mapping the input features into a higher-dimensional space, the kernel function computes the dot product between the data points in the feature space, implicitly transforming them into a higher-dimensional space.\n",
    "\n",
    "3. **Soft Margin Classification:**\n",
    "   In cases where the data is not linearly separable, SVM allows for soft margin classification. This means that some misclassification of training examples is tolerated to achieve a wider margin and better generalization to unseen data. The regularization parameter \\( C \\) controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    " \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b0b23-3372-43f7-a1fd-54e9d72e89db",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>The fit() method explained</b></summary>\n",
    "    \n",
    "In scikit-learn, the `fit()` method is used to train a machine learning model on a given dataset. Here's a breakdown of what the `fit()` method does:\n",
    "\n",
    "1. **Model Initialization**: Before calling `fit()`, you create an instance of a scikit-learn estimator (model) object, specifying any hyperparameters that control the model's behavior.\n",
    "\n",
    "2. **Training Data**: You pass the training data (features and corresponding labels, if applicable) to the `fit()` method. This data is used to train the model, meaning the model adjusts its internal parameters to minimize the difference between its predictions and the true labels (in supervised learning) or to capture patterns in the data (in unsupervised learning).\n",
    "\n",
    "3. **Training Process**: The `fit()` method triggers the training algorithm specific to the chosen estimator. During training, the model learns from the provided data by updating its internal parameters based on the optimization objective and the patterns observed in the data.\n",
    "\n",
    "4. **Model Adaptation**: As the model is trained, its internal parameters are adjusted to fit the training data. For example, in linear regression, the coefficients are calculated to minimize the residual sum of squares, while in decision trees, the splits are determined to maximize information gain or minimize impurity.\n",
    "\n",
    "5. **Return Value**: The `fit()` method typically returns the trained estimator object itself. This allows you to chain method calls or access attributes of the trained model for further analysis or prediction.\n",
    "\n",
    "6. **Optional Parameters**: Some estimators may have optional parameters that control the training process, such as the number of iterations, convergence criteria, or regularization strength. These parameters can be specified when calling the `fit()` method to customize the training behavior.\n",
    "\n",
    "7. **Error Handling**: If there are any issues during the training process, such as invalid input data or numerical instability, the `fit()` method may raise exceptions or issue warnings to alert the user.\n",
    "\n",
    "In summary, the `fit()` method in scikit-learn is the primary interface for training machine learning models. It encapsulates the training logic specific to each estimator and is a fundamental step in the machine learning workflow.\n",
    "\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f2247-0c24-401f-aeb7-ea3a312e1e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data and populate X and y\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_model = SVC()\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fda6f-7ad8-4b52-9ce8-ced5078107dc",
   "metadata": {},
   "source": [
    "## Exercise 1: \n",
    "\n",
    "Load the Breast Cancer dataset from `sklearn.datasets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61900d21-6fa0-4e0d-8658-dfa17c9b4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d870f07-d906-4d08-a4e8-0cb54f2b29a0",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "\n",
    "Train a Decision Tree classifier on the Breast Cancer dataset and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a9180-472d-44e9-918a-5805959d7edc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Overview</b></summary>\n",
    "    \n",
    "The DecisionTreeClassifier in scikit-learn (sklearn) is a popular machine learning model used for classification tasks. It builds a decision tree from the training data, which can be visualized as a flowchart-like structure. At each node of the tree, the algorithm makes a decision based on a feature's value, leading to a splitting of the data into different branches. This process continues recursively until a stopping criterion is met, such as reaching a maximum depth, minimum samples per leaf, or other user-defined conditions.\n",
    "\n",
    "Here's a breakdown of some key aspects:\n",
    "\n",
    "1. **Splitting Criterion**: The decision tree algorithm determines the best way to split the data at each node. Common splitting criteria include Gini impurity and entropy, which measure the impurity of a node (i.e., how mixed the classes are).\n",
    "\n",
    "2. **Tree Pruning**: Decision trees have a tendency to overfit the training data, capturing noise along with the underlying patterns. Pruning techniques are used to prevent this by removing parts of the tree that do not provide significant predictive power. In scikit-learn, this can be controlled using parameters like `max_depth`, `min_samples_split`, and `min_samples_leaf`.\n",
    "\n",
    "3. **Handling Categorical and Numeric Data**: DecisionTreeClassifier in scikit-learn can handle both categorical and numeric features. It uses techniques like binary splitting for numeric features and one-hot encoding for categorical features.\n",
    "\n",
    "4. **Performance Metrics**: Once trained, the DecisionTreeClassifier can be used to make predictions on new data. Common performance metrics for classification tasks include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC).\n",
    "\n",
    "5. **Feature Importance**: Decision trees also provide a measure of feature importance, indicating which features were most influential in making decisions. This can be useful for feature selection and understanding the data.\n",
    "\n",
    "Overall, DecisionTreeClassifier is a versatile and widely used algorithm due to its simplicity, interpretability, and ability to handle both numerical and categorical data. However, it's important to tune its parameters carefully to prevent overfitting and achieve optimal performance.\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bbc31-95a1-4e78-8f48-25e8e0076945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b269d-fc2a-4522-9324-e69b8381fcbb",
   "metadata": {},
   "source": [
    "## Exercise 3: \n",
    "\n",
    "Use K-Means clustering on the Iris dataset and visualize the clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f476bb-7806-4202-b126-312e783104f5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Overview</b></summary>\n",
    "    \n",
    "The K-means clustering algorithm is a popular unsupervised learning method used for partitioning a dataset into K distinct, non-overlapping clusters. Here's how it works:\n",
    "\n",
    "1. **Initialization**: The algorithm starts by randomly initializing K cluster centroids (points in the feature space) within the domain of the data. These centroids represent the centers of the initial clusters.\n",
    "\n",
    "2. **Assignment Step**: Each data point in the dataset is assigned to the nearest centroid, forming K clusters. The \"nearest\" centroid is typically determined using a distance metric such as Euclidean distance.\n",
    "\n",
    "3. **Update Step**: After assigning each data point to a cluster, the centroids are updated by computing the mean of all the data points assigned to each cluster. This moves the centroids to new locations within the feature space.\n",
    "\n",
    "4. **Iteration**: Steps 2 and 3 are repeated iteratively until a stopping criterion is met. Common stopping criteria include a maximum number of iterations or when the centroids no longer change significantly between iterations.\n",
    "\n",
    "5. **Convergence**: Eventually, the centroids converge to stable positions, and the algorithm stops. At this point, the data points are clustered in a way that minimizes the sum of squared distances between each data point and its corresponding centroid, known as the \"within-cluster sum of squares\" or \"inertia.\"\n",
    "\n",
    "6. **Final Clustering**: The final clusters are formed based on the converged centroids.\n",
    "\n",
    "Key considerations and characteristics of K-means clustering:\n",
    "\n",
    "- **Number of Clusters (K)**: One of the critical parameters in K-means clustering is the number of clusters, K. It's often determined using domain knowledge or by using techniques such as the elbow method or silhouette analysis to find the optimal value of K.\n",
    "\n",
    "- **Cluster Centroids**: The centroids represent the centers of the clusters and are updated iteratively to minimize the within-cluster sum of squares.\n",
    "\n",
    "- **Initialization Sensitivity**: K-means clustering is sensitive to the initial positions of the centroids. Different initializations may lead to different final clustering results.\n",
    "\n",
    "- **Scalability**: K-means clustering is computationally efficient and scales well with large datasets. However, it may struggle with high-dimensional data or clusters of varying sizes and densities.\n",
    "\n",
    "- **Assumptions**: K-means assumes that clusters are spherical and of similar size, which may not always hold true in real-world datasets. As a result, it may produce suboptimal results for non-linear or irregularly shaped clusters.\n",
    "\n",
    "Overall, K-means clustering is widely used for data exploration, pattern recognition, and segmentation tasks, but it's essential to interpret the results carefully and consider its limitations.\n",
    "\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a079375-0f24-43d8-b692-90825fcb6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa3ce3-5b59-486e-ad3e-969f4d4a6382",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Scikit-learn is a powerful library for machine learning in Python, offering a wide range of algorithms and tools for various tasks. By following this tutorial and practicing the exercises, you'll gain a good understanding of how to use scikit-learn effectively for building and evaluating machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be27438-53db-407a-89bf-78f436ba2176",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Instructor Notes</b></summary>\n",
    "\n",
    "I wanted to have the sklearn datasets already downloaded if the class is unable to get the data. There were a few changes made to get everything to work.\n",
    "    \n",
    "The 'species' column was not showing up. I followed this stackoverflow.com link.\n",
    "    \n",
    "https://stackoverflow.com/questions/69821857/iris-dataset-not-showing-species-column\n",
    "    \n",
    "How to create the species column from target and target_names columns?\n",
    "    \n",
    "You just need a dict mapping to replace 0 by 'setosa', 1 by 'versicolor' and 2 by 'virginica'. Use enumerate to create a list of tuples [(0, 'setosa'), (1, 'versicolor), (2, 'virginica')] then dict to convert as a dictionary.\n",
    "\n",
    "Now Series.map will map the corresponding values.\n",
    "    \n",
    "Then I was getting the following error:\n",
    "    \n",
    "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
    "  y = column_or_1d(y, warn=True)\n",
    "    \n",
    "From this link:\n",
    "    \n",
    "https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected   \n",
    "    \n",
    "    \n",
    "When using csv and avoid this warning, use y_train.values.ravel(). \n",
    "    \n",
    "The problem was that the labels were in a column format while it expected it in a row. use np.ravel()  \n",
    "    \n",
    "Here is the code to save the dataset to a csv:\n",
    "    \n",
    "```python\n",
    "# This is how the dataset was originally Loaded from sklearn and saved to a csv\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# In order to get the 'species' column had to use the following code\n",
    "df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                  columns= iris['feature_names'] + ['target']).astype({'target': int}) \\\n",
    "       .assign(species=lambda x: x['target'].map(dict(enumerate(iris['target_names']))))\n",
    "\n",
    "df.to_csv('iris.csv', header = True, index = False)\n",
    "```\n",
    "    \n",
    "Here is the code to read the csv:\n",
    "    \n",
    "```python    \n",
    "# Read from csv and populate X and y\n",
    "iris = pd.read_csv('iris.csv')\n",
    "\n",
    "X = df[['sepal length (cm)', 'sepal width (cm)','petal length (cm)', 'petal width (cm)']]\n",
    "y = df[['species']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_model = SVC()\n",
    "\n",
    "# When using csv and avoid this warning, use y_train.values.ravel()\n",
    "svm_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
